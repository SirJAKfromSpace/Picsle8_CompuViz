{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "\n",
    "from pytorch_datasetloader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # A bunch of convolutions one after another\n",
    "        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(128), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 4, padding=1),\n",
    "                    nn.InstanceNorm2d(512), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        # FCN classification layer\n",
    "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.model(x)\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features)  ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block       \n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc, 64, 7),\n",
    "                    nn.InstanceNorm2d(64),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(64, output_nc, 7),\n",
    "                    nn.Tanh() ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#using cuda if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "netD_A = Discriminator(3)\n",
    "netD_A = netD_A.to(device)\n",
    "netD_B = Discriminator(3)\n",
    "netD_B = netD_B.to(device)\n",
    "\n",
    "netG_A2B = Generator(3,3)\n",
    "netG_A2B = netG_A2B.to(device)\n",
    "netG_B2A = Generator(3,3)\n",
    "netG_B2A = netG_B2A.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 images from the dataset\n",
      "350 images from the dataset\n"
     ]
    }
   ],
   "source": [
    "batch_size_Realpix = 10\n",
    "batch_size_mixed = 10\n",
    "dataset_len =  350\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = 'RealFaces_350'\n",
    "trainloader_nonpixel, validloader_nonpixel = get_loaders(path,split_perc=1.0,batch_size=batch_size_Realpix)\n",
    "\n",
    "path = 'PixelFaces_350'\n",
    "trainloader_pixel, validloader_pixel = get_loaders(path,split_perc=1.0,batch_size=batch_size_mixed)\n",
    "\n",
    "\n",
    "trainiter_nonpixel = iter(trainloader_nonpixel)\n",
    "trainIter_pixel = iter(trainloader_pixel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "# Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CycleGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch No [10/2500] Discriminator Loss: 0.0530, Generator Loss: 6.4598\n",
      "Epoch No [20/2500] Discriminator Loss: 0.1142, Generator Loss: 5.6953\n",
      "Epoch No [30/2500] Discriminator Loss: 0.0715, Generator Loss: 5.1546\n",
      "Epoch No [40/2500] Discriminator Loss: 0.2964, Generator Loss: 6.8656\n",
      "Epoch No [50/2500] Discriminator Loss: 0.0362, Generator Loss: 6.0577\n",
      "Epoch No [60/2500] Discriminator Loss: 0.0837, Generator Loss: 5.3438\n",
      "Epoch No [70/2500] Discriminator Loss: 0.0568, Generator Loss: 4.2689\n",
      "Epoch No [80/2500] Discriminator Loss: 0.0461, Generator Loss: 4.5756\n",
      "Epoch No [90/2500] Discriminator Loss: 0.0467, Generator Loss: 5.4504\n",
      "Epoch No [100/2500] Discriminator Loss: 0.0679, Generator Loss: 3.5014\n",
      "Epoch No [110/2500] Discriminator Loss: 0.0578, Generator Loss: 4.1587\n",
      "Epoch No [120/2500] Discriminator Loss: 0.0239, Generator Loss: 3.6526\n",
      "Epoch No [130/2500] Discriminator Loss: 0.0636, Generator Loss: 4.0301\n",
      "Epoch No [140/2500] Discriminator Loss: 0.0768, Generator Loss: 3.9887\n",
      "Epoch No [150/2500] Discriminator Loss: 0.1068, Generator Loss: 3.2747\n",
      "Epoch No [160/2500] Discriminator Loss: 0.0414, Generator Loss: 5.5154\n",
      "Epoch No [170/2500] Discriminator Loss: 0.0411, Generator Loss: 3.1830\n",
      "Epoch No [180/2500] Discriminator Loss: 0.0218, Generator Loss: 3.5609\n",
      "Epoch No [190/2500] Discriminator Loss: 0.0440, Generator Loss: 3.8442\n",
      "Epoch No [200/2500] Discriminator Loss: 0.0238, Generator Loss: 3.4770\n",
      "Epoch No [210/2500] Discriminator Loss: 0.0453, Generator Loss: 2.9373\n",
      "Epoch No [220/2500] Discriminator Loss: 0.0457, Generator Loss: 2.7767\n",
      "Epoch No [230/2500] Discriminator Loss: 0.0858, Generator Loss: 2.7717\n",
      "Epoch No [240/2500] Discriminator Loss: 0.0840, Generator Loss: 2.8349\n",
      "Epoch No [250/2500] Discriminator Loss: 0.0575, Generator Loss: 3.1321\n",
      "Epoch No [260/2500] Discriminator Loss: 0.0089, Generator Loss: 2.9288\n",
      "Epoch No [270/2500] Discriminator Loss: 0.0834, Generator Loss: 2.6274\n",
      "Epoch No [280/2500] Discriminator Loss: 0.0513, Generator Loss: 2.8824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ffbf5dda6171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# Total loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mloss_G\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_identity_A\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_identity_B\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_GAN_A2B\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_GAN_B2A\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_cycle_ABA\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_cycle_BAB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mloss_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\monogatari\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\monogatari\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Patch Version\n",
    "\n",
    "# Train the model\n",
    "total_step = len(trainiter_nonpixel)\n",
    "d_loss_list = []\n",
    "g_loss_list = []\n",
    "acc_list = []\n",
    "num_epochs = 2500\n",
    "sample_imflag = 0\n",
    "imsize = 128\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# A: Non-pixel, B: pixel\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    trainiter_nonpixel = iter(trainloader_nonpixel)\n",
    "    trainIter_pixel = iter(trainloader_pixel)\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        real_pixel, input_lbls = trainIter_pixel.next()\n",
    "        real_nonpixel, out_lbls = trainiter_nonpixel.next()\n",
    "        \n",
    "        \n",
    "        real_pixel = real_pixel.to(device)\n",
    "        real_nonpixel = real_nonpixel.to(device)\n",
    "        \n",
    "        target_real = Variable(Tensor(len(input_lbls)).fill_(1.0), requires_grad=False)\n",
    "        target_fake = Variable(Tensor(len(out_lbls)).fill_(0.0), requires_grad=False)\n",
    "        \n",
    "        \n",
    "        netG_A2B.zero_grad()\n",
    "        netG_B2A.zero_grad()\n",
    "        \n",
    "        #self identity loss:\n",
    "        \n",
    "        pixel_gen = netG_A2B(real_pixel)\n",
    "        loss_identity_B = criterion_identity(pixel_gen, real_pixel)*20.0\n",
    "        \n",
    "\n",
    "        nonpixel_gen = netG_B2A(real_nonpixel)\n",
    "        loss_identity_A = criterion_GAN(nonpixel_gen, real_nonpixel)*20.0\n",
    "\n",
    "        \n",
    "        # GAN loss\n",
    "        actual_pixel_gen = netG_A2B(real_nonpixel)\n",
    "        pred_pixel = netD_B(actual_pixel_gen)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_pixel, target_real)\n",
    "        \n",
    "\n",
    "        actual_nonpixel_gen = netG_B2A(real_pixel)\n",
    "        pred_nonpixel = netD_B(actual_nonpixel_gen)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_nonpixel, target_real)        \n",
    "        \n",
    "        \n",
    "        # Cycle loss\n",
    "        recovered_nonpixel = netG_B2A(actual_pixel_gen)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_nonpixel, real_nonpixel)*10.0\n",
    "\n",
    "        recovered_pixel = netG_A2B(actual_nonpixel_gen)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_pixel, real_pixel)*10.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Total loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        \n",
    "        ###### Discriminator A ######\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_A(real_nonpixel)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        actual_nonpixel_gen = netG_B2A(real_pixel)\n",
    "        pred_fake = netD_A(actual_nonpixel_gen)\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        \n",
    "        ###### Discriminator B ######\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_B(real_pixel)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        actual_pixel_gen = netG_A2B(real_nonpixel)\n",
    "        pred_fake = netD_B(actual_pixel_gen)\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D_B.step()        \n",
    "        \n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "      \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch No [{}/{}] Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(epoch+1,num_epochs,loss_D_B.item(),loss_G.item()))\n",
    "        #appending loss every 10 epochs\n",
    "        d_loss_list.append(loss_D_B.item())\n",
    "        g_loss_list.append(loss_G.item())\n",
    "    \n",
    "        # Save fake images\n",
    "        fake_images = actual_pixel_gen.view(actual_pixel_gen.size(0), 3, imsize, imsize)\n",
    "        save_image(fake_images.data, 'images/GAN_IO/fakes/fake_images-%d.png' %(epoch+1))\n",
    "        \n",
    "        \n",
    "        # Save input images\n",
    "        if sample_imflag == 0:\n",
    "            inputImages = real_nonpixel.view(real_nonpixel.size(0), 3, imsize, imsize)\n",
    "            save_image(inputImages.data, 'images/GAN_IO/input_images/inputImages-%d.png' %(epoch+1))\n",
    "            sample_imflag = 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Save models checkpoints\n",
    "torch.save(netG_A2B.state_dict(), 'GAN_OUTs/netG_A2B_HigherCyclce.pth')\n",
    "torch.save(netG_B2A.state_dict(), 'GAN_OUTs/netG_B2A_HigherCylce.pth')\n",
    "torch.save(netD_A.state_dict(), 'GAN_OUTs/netD_A_HigherCycle.pth')\n",
    "torch.save(netD_B.state_dict(), 'GAN_OUTs/netD_B_Cylce.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-82732a38ec26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_loss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd_loss_list' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Save models checkpoints\n",
    "# torch.save(netG_A2B.state_dict(), 'GAN_OUTs/netG_A2B.pth')\n",
    "# torch.save(netG_B2A.state_dict(), 'GAN_OUTs/netG_B2A.pth')\n",
    "# torch.save(netD_A.state_dict(), 'GAN_OUTs/netD_A.pth')\n",
    "# torch.save(netD_B.state_dict(), 'GAN_OUTs/netD_B.pth')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.plot(d_loss_list)\n",
    "plt.plot(g_loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('x10 Epochs')\n",
    "plt.savefig('images/GAN_LOSS_PLOT/GAN_Loss_CycleGAN.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 images from the dataset\n",
      "torch.Size([10, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "G = Generator(3,3)\n",
    "G = G.to(device)\n",
    "G.load_state_dict(torch.load('GAN_OUTs/netG_A2B.pth'))\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "path = 'RealFaces_350'\n",
    "trainloader_pixel, _ = get_loaders(path,split_perc=1.0,batch_size=batch_size,num_workers=0)\n",
    "\n",
    "testImages = iter(trainloader_pixel).next()\n",
    "\n",
    "testimg = testImages[0].to(device)\n",
    "print(testimg.shape)\n",
    "\n",
    "output = G(testimg)\n",
    "\n",
    "fake_images = output.view(output.size(0), 3, 128, 128)\n",
    "f = fake_images.detach().cpu().numpy()\n",
    "save_image(fake_images.data, 'images/GAN_IO/CycleGAN_output.png')\n",
    "\n",
    "testImages = testimg.view(testimg.size(0), 3, 128, 128)\n",
    "t = testImages.detach().cpu().numpy()\n",
    "save_image(testimg.data, 'images/GAN_IO/CycleGAN_input.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
